## 自我介绍
面试官你好，首先感谢您来抽空给我做这个面试，我叫郭超。本科毕业于武汉大学电子信息工程专业。目前是9年工作经验，毕业以后就一直在华为。
5年的Java的开发和设计经验，其中2年的大型项目需求分析和设计落地经验。
整个在华为的经历大致分为三段：
    第一阶段是入职后的前5年，在做系统测试和软件开发工作，在这个阶段主要是系统业务开发，参与了视频云项目的云化改造过程，核心做的事情有
        第一个是服务化的适配 包括配置中心集成，服务化RPC适配，日志调用链改造。
        改造完成后，负责公共平台组件的需求开发，包括引入flume做数据集消费，以及检索算法检索的分布式改造。
    这一阶段掌握了微服务的框架，服务化拆分改造，配置中心改造，业务开发等能力    

    第二段是作为视频云项目的软件总工/committer(代码看护者)，看护代码合入质量，以及负责软件演进优化和疑难问题定位等职责。此阶段核心事务包括
        1、主导了SDK模块的代码重构：通过Structure101分析代码架构，解决代码依赖换乱，开发效率低的问题
        2、算法插件框架适配：设计一套插件框架，通过模版定义插件编码、类型、调用接口，来支持插件的快速集成
        3、提升人脸检索性能：通过设计跳表结构加快索引查找效率，整体减少90%检索性能
        4、优化MongoDB分页查询：MongoDB通过简单的skit的方式性能比较差，通过时间轴的方案优化MongoDB的分页查询性能
        5、视图设备接入路数的性能提升：通过自定义AciiCode以及弱引用WeakHashMap缓存，减少内存占用
        另外还有一系列的疑难问题定位等，以及内存优化工作
    这一阶段作为整体项目的软件总工，从整个系统出发，分析整个系统的瓶颈和改进点，并通过团队协作，优化系统的性能和稳定性，这一段经历我认为也是我整个工作挑战比较大的一个阶段

    第三阶段是作为视频云项目的SE,负责一些项目的方案设计落地，设计的方案包括分布式存储纳管和迁移方案，全息人像方案设计，静态库20亿底库的高并发检索设计。
    这一阶段主要锻炼了系统方案的分析，从需求分析到方案设计，从需求的性能规格，功能，可靠性，易用性，安全等方面进行分析，个人能力上也具备了系统性思维
    
    总结在个人能力上具备几个通用的能力：
    扎实的编码基础：在代码规范，日志打印，并发编程，代码性能(内存使用)，开发者测试等
    具备良好的分析和解决问题能力：具有多次现网重大事故/疑难问题定位经验，比如进程core掉，fullGC，性能优化等
    具备良好的沟通和团队合作，以及较好的执行力
    掌握常用中间件的原理，具备系统方案分析和实现能力

## 项目经历
### k8s经历自定义topoKey
对于工作负载反亲和来说，使用requiredDuringSchedulingIgnoredDuringExecution规则时， Kubernetes默认的准入控制器 
LimitPodHardAntiAffinityTopology要求topologyKey字段只能是kubernetes.io/hostname。如果您希望使用其他定制拓扑逻辑，
可以更改或者禁用该准入控制器。
https://support.huaweicloud.com/intl/zh-cn/basics-cce/kubernetes_0018.html

1.21以前版本的集群中，Pod中获取Token的形式是通过挂载ServiceAccount的Secret来获取Token，这种方式获得的Token是永久的。
该方式在1.21及以上的版本中不再推荐使用，并且根据社区版本迭代策略，在1.25及以上版本的集群中，ServiceAccount将不会自动创建对应的Secret。
1.21及以上版本的集群中，直接使用TokenRequest API获得Token，并使用投射卷（Projected Volume）挂载到Pod中。
使用这种方法获得的Token具有固定的生命周期，并且当挂载的Pod被删除时这些Token将自动失效。详情请参见Token安全性提升说明。

如果您在业务中需要一个永不过期的Token，您也可以选择手动管理ServiceAccount的Secret。
尽管存在手动创建永久ServiceAccount Token的机制，但还是推荐使用TokenRequest的方式使用短期的Token，以提高安全性。
Token安全机制升级



dispatcher支持手动后台的扩缩容，需要改造自动扩缩容，怎么改造？

Token的安全机制怎么做的？

k8s的Operator怎么开发？

系统压测怎么做？

推理系统功能，怎么做的？

go安装，dispatcher的java到go的改造

灰度升级

wiremockserver

sidecar

模型动态加载


### 异构调度规格设计
通过k8s支持拉起GPU,DPU等资源的容器，并按照规格调度，开发k8s operator满足



### 讲一下分布式存储纳管和迁移的方案
客户需求：我所在的分布式存储部门主要是靠销售分布式存储产品的，在拓展中有大部分的客户是已经有了对象存储，又由于客户不会轻易的说要换掉
已经运行在现网中的设备，所以需要能够兼容客户的对象存储系统，因此，我们需要支持能够管理起来旧存储系统，这个方案叫对象纳管迁移需求

需求分析：怎么样实现纳管和迁移的方案呢？纳管则是能够将旧设备统一管理，迁移则是将数据 能够迁移到分布式存储系统中。 我们设计支持了是三种模式：
    纳管模式：
        域名风格：DNS解析模式
        路径风格：代理写入
    迁移模式：
        数据迁移：停止解析，停止代理写入，数据单写选择读
    那要支持的功能：
    1、设备管理和纳管：首先设备添加到系统中，一次添加一个桶，进行纳管 
    2、DNS解析模式：DNS解析模块接收纳管桶信息，在进行DNS解析时要返回NS记录
    3、路径风格的纳管：代理的数据写入
    4、迁移模式：一旦启动数据迁移任务，那么此时在访问数据时，要支持数据单写 选择读
    5、数据的迁移完成后，所有的数据就都在新存储上访问

方案设计考虑到的点包括：
易用性上，一次添加一个桶，效率太低，开发上设计了导入工具，支持多个桶一次性纳管
数据的list性能是阻塞点，遍历获取所有文件效率低：实现了按照字母序列分节点获取数据，但是仅仅按照字母序无法，可能存在数据分布不均的问题，我们支持用户自定义分组list能力，比如客户知道数据的分布比例，
手动定义分组边界，以让数据能够均匀分布，list获取所有数据，效率更高
还有方案上状态机的设计，比如针对一个桶的状态有多个：未纳管(同名桶冲突，纳管失败，有同名桶时，无法解析路由位置)，已纳管(不允许在新存储创建同名桶)，迁移中(数据要在新存储侧访问，桶的映射)，
迁移失败(需要支持重新启动)

### 讲一下全息人像设计方案
全息人像代表的意思是一个人的完整画像 ，在视频云的项目里，一个人的信息主要包括个人的结构化信息，个人的特征和非结构化信息(图片、抓拍)，我们要做的全息人像能力主要有
一个是要支持结构化数据的检索，非结构化数据检索，返回此人的整体档案
那这个里面的方案要怎么实现：
先说数据结构：
一个是名单库，这个数据是在mongo中的，主要记录个人信息，以及底库的图片url和底库特征，
另外一个是档案库分开两部分：
一部分是档案数据表，档案库表和档案人员数据表存储在mongo和solr中；
    档案库表：档案库id，创建时间，描述等等
    档案人员数据表：这个就是我们真金白银创建出来的表，这个档案人员的id，名单库的id(有名档、无名档)，人的身份信息，档案库的id，创建时间，更新时间
一部分是档案特征表，档案特征表存储在检索存储系统中，档案特征表主要保存档案id和档案代表特征的关系
原有的动态库里增加字段：过脸归属哪个档案，是否命中
好了，基于这些个数据，我们要做支持起来的流程
一个是支持人脸的在离线比对：在离线比对，通过客户端下发聚档任务，检索模块会触发，将聚档的名单库分配到不同的聚档容器中做比对，每个聚档容器负责一部分的聚档任务
一个是支持档案的检索：检索能力包括，以图搜档

支持初始建档：原始的人脸档案由客户端下发到SDK后，SDK异步创建初始档案，每次100个人员信息，将初始档案插入到MongoDB中，静态库的此人标记已建档，将此档案数据推送到kafka中
flume支持消费kafka档案数据，并插入档案特征表，tserver要计算中心特征。对应有一些运维特性要支持，手动建档/删除档案
支持配置实时聚档任务：指定相机和时间段，档案库，质量分阈值，进行归档任务配置；聚档MCS收到聚档任务后，从Tserver加载所有档案，并加载到聚档算法中
全息人像的实时归档：MCS聚档模块消费数据后，调用MCS检索模块的1vN检索，检索到top1，命中则为有名档，将add消息推送到kafka，flume更新solr中的过脸档案库
未命中则为无名档，同样更新档案库未命中
支持离线的聚档任务：
    离线聚档设计的调度策略，有两种，
        一种是固定时间聚档，这种适合小数据量的数据，一天1000W过脸数据以内
        一种是定时定量聚档，过脸数据量1000W以上，比如长沙的项目数据量在1.35亿
聚档模块要支持档案合并：当天档案合并到历史无名档或者有名档，先Update档案特征数据，再Add档案数据
    对于历史无名档合并到有名档，先发送档案合并消息，再发送档案特征数据Update消息，最后删除无名档案

档案ID检索：档案id直接去mongo中检索对应档案id，返回数据中也包括档案封面url，从mp中返回档案封面
档案ID检索过脸数据：从solr中检索 id&time&camera(coordinate)，再去mongo取详细信息，最后从mp取图片

方案设计的难点：
初始档案入库，写入时间太长？
支持分布式档案入库，每个sdk节点均触发MongoDB的静态库数据获取，插入到MongoDB中，提升性能；通过指定分片的方式
db.collection.find().shard("shard1")

节点故障恢复方案设计复杂，数据一致性如何保证：
通过分布式事务一致性保证：
SDK既要分配CMU用哪个VA进行视频流分析，又要通知MCS启动聚档任务，一旦视频流的分析任务分配失败，那MCS的聚档任务就不会有数据，但是又由于不需要强一致性的任务分配成功
SDK直接配置资源分配和聚档任务给各个模块，如果其中一个失败，通过记录任务的详细状态，支持能够对操作进行重试，重试失败，则针对2个模块全部回退。回退的任务最终会发布到MQ中
确保这个任务能够完全回退掉，一旦模块消费了回退，再发布回退成功的消息，确保SDK能够感知回退成功

还有我们的管理面的系统部署流程：设计了容器内资源，检查，通知，回退，把每个状态记录，保证数据的一致性
还有配置中心：档案的代表特征的封面图像过期，这个从配置中心获取留存期

### 软件优化
性能优化：
1、代码Structure101重构整体SDK代码，包括工具层，基础服务层，数据服务层，业务调度层。通过structure101分析代码依赖关系，识别代码架构问题，进行代码重构
2、算法插件化框架适配：设计一套插件化框架，通过模版定义插件编码，类型，调用接口，支持算法插件，设备介入插件，驱动插件系统部署
3、检索性能优化：检索时间分析优化，通过跳表的机制实现范围检索的速度提升。特征值的数据的检索维度有2个一个是cameraid，一个是时间，索引结构先通过map的key值用的是camearaid
    而时间范围的检索一般是通过年月日，所以设计跳表的结构也刚好合适，第一层是划分到年，第二层划分到月，第三层是日，数据结构体包含日期，特征数组，特征id数组
4、MongoDB查询性能优化：时间轴优化分页查询性能：我们有动态库的数据页面展示，最开始是通过计算总数/每页展示量，通过skip.limit形式来展示数据量。但是系统后来支持的动态库
    已经上亿的数据，基本上比较靠后的数据查询性能很差，如果跳到后面需要很长的时间。由于这种大数据量的查询本身也不存在使用场景，所以这里我们做了时间轴的优化：通过展示10页数据，以及上一页
    下一页的方式做，缓存每页的起始数据的id，每次在起始id上，skip和limit，小范围skip提升性能
5、内存优化：PCG设备介入模块频繁fullGC，我通过分析堆内存的使用情况，确定String占用的内存比例高，通过自定义AsciiCode以及弱引用WeakHashMap缓存，
减少内存占用，提升50%的设备介入数量，这个是通过Netty源码得到的灵感
    通过Jprofile分析当前的数据有多少重复对象，查看引用关系，90w的设备对象引用了500w的String对象实例，设备对应不应该有这么多的string引用关系，进一步
    分析发现字符串有很多重复字符串，以及类似字符串。
    优化手段有2个：
    1、缩减字符串的编码占用空间：所有字符串的编码都在ascii码范围内，在做字符串优化时，之前看的netty源码里有在用AsciiString的字符串结构，为什么不用原生的String
        JDK8上String使用的char数组来保存的字符串，每个字符是2字节，1个20字符的字符串占比约8+20*2=48字节，JDK9以后使用Lattein1或者UTF来编码，一个字符是1-2个字节
        空间会减少不少。那么在我们的系统里统一使用的jdk8，又由于没有lattein1编码，刚刚讲了设备全是Ascii范围内的，那么考虑使用Acsii编码的字符串方式来减少字符串使用量
        每个Ascii编码只占用一个字节
    2、缩减重复字符串：分析到重复的字符串主要是设备的编码的ParentID以及域编码，将这些改为引用来解决
        1> 开启指针压缩，使用引用也会占用空间，8字节->4字节
        2> 缓存如何清理：比如我们添加的设备移除了，那么其对应的引用已经不再使用了，缓存要能够自动移除。这个里面我们参考ThreadLocal里的缓存方案
            所以这里引入弱引用，AsciiCode里有一个静态变量，WeakHashMap，将引用不再使用后，通过gc自动触发回收

疑难问题：
1、内存溢出触发FullGC：通过分析平台源码，以及GC日志分析，确定服务中心代码实现的问题，服务类型实例化，导致内存持续增大
2、多线程修改任务状态：触发乐观锁异常，通过改造死锁解决，for update字段加悲观锁，乐观锁一般有两种实现方式，version和时间戳
3、还有其他比较多的疑难问题：比如一些网络socket异常，双机hacs软件bug，httpd服务core等问题

#### 找工作的原因
    视频云项目目前不再演进，工作内容发生变化，不符合个人发展规划。我个人对XX领域很有兴趣，真诚希望能够加入你们公司

#### 有互联网项目经验么
    我所做的项目是传统的ToB云化项目，能力上虽然没有互联网的规模，但架构上基本按照互联网的服务化架构来设计，我对大型的互联网架构和设计逻辑也比较熟悉：
比如：xxxx

#### 面试注意
1、主观能动性：面试中要自信侃侃而谈，做好准备将自己要展现给面试官的  
    能力包括：系统设计，业务开发，疑难问题定位，协作拉通能力，技术储备(隐形，这个是面试官会问到)

### 运维指标的监控方案
### 系统调度逻辑
1、容器化部署调度算法
2、SDK的任务调度
### 数据库查询性能优化

### 全局文件系统方案设计
客户需求：崖州湾，中国移动，深圳超算等项目都有跨域的数据统一访问的诉求，多地域访问相同的命名空间，数据能够支持按需流动
需求分析：数据流动分为2个层面，一个是元数据的流动，一个是数据的流动，并且我们需要在全局提供统一的命名空间。所以大的需求上包括：
全局命名空间的配置管理；元数据的同步方案；数据的访问/流动方案，达成元数据变更的秒级更新，实现秒级的全局数据可视方案
配置上要做的事情：
1、S3/NAS协议的远端设备管理；
2、配置GFS Group，以及GFS Group对应的命名空间关联
3、选择一个站点作为metaStore 元数据的订阅通知桶，桶里面包含全局命名空间访问的账号，存储池，站点SN号；存储元数据的变更日志，提供通知服务
4、元数据发布和订阅：将元数据日志打包为对象文件发布到metastore的s3桶中，支持全量，增量方式发布；订阅某个s3桶的某种规则的对象文件，比如：GFS_GROUPID_SN_FS_ID的put事件
5、数据流动的策略
元数据的流动方案：我们已经做了基于s3bucket的通知事件特性，基于s3 bucket的事件特性来支持元数据的广播：
1、元数据的发布：按照命名空间或者dtree粒度，将本地集群的元数据日志按照固定规格打包为对象文件写入metastore的s3桶中
2、元数据订阅：在metastore站点订阅s3桶的对象文件变更通知事件，如：对象前缀名 GFS_GROUPID_GFSNAME_SN_FS_ID put事件
3、元数据回放：接收到metastore的元数据变更通知，向metastore获取元数据打包对象文件，解压解析后，根据元数据描述信息回放元数据日志，如：创建文件等
数据的IO方案：
元数据的订阅回放实现思路：
1、联邦内各集群的元数据变更在本地生成FSImage文件，上传到metastore对象存储
2、metastore通知其他所有集群
3、其他集群获取对应的FSImage文件，回放到本地，最终保持统一的元数据视图
有几种元数据的发布模式：
全量
全量+增量（周期发布）
增量（实时）
全量+增量（实时）

扫描全量时，需要生成checkpoint的进度的描述信息：
checkpointid：xxx
log_base_index：vnode1:sn1,vnode2:sn2,vnode3:sn3
checkpoint_state: sealed or inprogress

元数据回放：
根据vnodehash到对应的节点记录事件flog
根据checkpoint获取当前sn，以及根据事件消费的sn1去对比，如果有残缺，则获取差异的sn1-sn，依次回放元数据，回放需要获取s3中的fsimage，然后根据文件目录，确认处理的节点

    元数据的处理存在冲突，有两个问题，1是元数据的操作存在依赖关系，但是回放由于乱序，依赖操作可能未执行，另外，多站点的操作可能冲突。采用最终一致性方案
        
    
    跨集群的数据读写机制：
        1、每个文件的扩展inode属性，要新增birthSiteId，birthUUID属性，代表文件归属哪个站点
        2、读写数据时，比较birthSite是否属于当前站点，如果不属于当前站点，通过跨集群的读写把数据缓存到本站点
        3、跨集群的缓存需要缓存淘汰机制
        4、跨集群写缓存时，需要支持异步刷新到owner站点
        5、

难点：
对象的多版本会被当做不同的文件元数据变化记录
元数据发布任务不支持采集快照信息，不感知快照回滚

广播的数据格式：
objectcreated:压缩数据，一批数据，订阅端要配置订阅消息，

### S3通知事件方案设计
1、通过配置面配置通知策略
2、VAF模块通过消费每个vnode的flog，将文件变更事件通知到订阅者，不保证时序性，保证vnode级别的时序性
3、支持最大32个消费者消费一个桶的事件
4、目前尚未支持消费者组，因此对于一个消费者异常后，消费的消息将积压，最大保留7天的flog

要求：消费者端要自身保持幂等性，消费者可以自己构建基于域名的集群，保证负载均衡，写入的事件基于key值的事件，可以保证幂等性，事件可能会通知两次